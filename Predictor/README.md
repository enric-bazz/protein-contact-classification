# Software implementation

## Overview
The current software implements our best performing model: an ensemble of OneVsRest XGBoost models trained for binary classifications on data containing additional features.
The software is located in the `Predictor` folder of the repository and supports two different ways of usage:
1. Terminal usage: run predictions through the terminal with a python script
2. Notebook usage: run predictions with a IPython Notebook in an interactive environment

---

## 1. Run from terminal
This software is designed to be run **after** executing the `calc_feature.py` and `cal_3di.py` scripts on the input `.pdb` file. It takes as input the `.tsv` files generated by those two scripts.

To run the software from the terminal:

### Step 1. Environment Setup
It is recommended to run the software inside a **dedicated virtual environment** to avoid dependency conflicts.

You can set up the environment using either of the following:

- `environment.yml` (for Conda users, tested)
- `requirements.txt` (for `pip` users)

If issues persist, a full list of required packages is provided:
- `argparse`: For command-line argument parsing.
- `pandas`: For handling data in tabular format.
- `Bio.PDB`: For downloading PDB files.
- `sklearn`: For preprocessing steps, such as label encoding and scaling.
- `xgboost`: For handling XGBoost model predictions.
- `numpy`: For numerical computations.
- `joblib`: For loading serialized model files.

> **Note:** The listed dependencies **do not** include those required to execute the scripts `calc_feature.py` and `cal_3di.py`, which must be run beforehand to generate the input `.tsv` files.

### Step 2. 
Ensure that the `Predictor/` folder is downloaded and placed in your **current working directory**.
   
   This folder contains:
   - `run_prediction.py` – main prediction script
   - `calc_additional_features.py` – extracts extra features for model input
   - `states.txt` – used for feature extraction
   - `onehot_encoder.pkl` – for data preprocessing
   - `models/xgboost/` – directory containing `.joblib` trained models

### Step 3. 
Make sure the required `.tsv` input files (produced by `calc_feature.py` and `cal_3di.py`) are also located in the current working directory.

The expected directory structure is:
```
working_dir/
├── your_input.tsv
├── Predictor/
│   ├── calc_additional_features.py
│   ├── onehot_encoder.pkl
│   ├── states_txt
│   └── models/
│       └── xgboost/
│           ├── HBOND.joblib
│           ├── IONIC.joblib
│           └── ... (other model files)
```

### Step 4. Run the prediction script

The `run_prediction.py` script supports three types of inputs:
1. **Single TSV file**
2. **Directory containing TSV files**
3. **ZIP file containing TSV files**

A usage example for each type of input is shown:
* **Single TSV file**:
``` bash
python run_prediction.py pdb_id.tsv  [output_dir]
```
* **Directory containing TSV files**:
``` bash
python run_prediction.py /path/to/tsv/directory [output_dir]
```
* **ZIP file containing TSV files**:
``` bash
python run_prediction.py input_data.zip [output_dir]
```

In all cases:
- The `[output_dir]` parameter is optional, if no output directory is specified, results will be saved in the current directory
- The script will process the input, calculate additional features, and generate predictions for different types of interactions (HBOND, IONIC, PICATION, PIHBOND, PIPISTACK, SSBOND, VDW)


The output results produced are:

1. **Features Output** (Generated by `calc_additional_features.py`):
   - Individual TSV files for each PDB structure in the output directory
   - A ZIP archive named `features_ring_extended.zip` containing all TSV files

2. **Preprocessed Data**:
   - File name: `preprocessed_features.tsv`
   - Contains the preprocessed features used for prediction

3. **Final Predictions**:
   - File name: `{pdb_id}_prediction.tsv` (where input_name is derived from the input file name)
   - Contains the final predictions with the following columns:
     - Original identifier columns (`s_ch`, `s_resi`, `s_ins`, `s_resn`, `t_ch`, `t_resi`, `t_ins`, `t_resn`)
     - All features columns used for the prediction
     - `Interaction`: List of predicted interaction types
     - `score`: List of corresponding confidence scores for each predicted interaction

Example file structure after running:
```
output_directory/
├── features_ring_extended.zip
├── pdb_id_1.tsv
├── pdb_id_2.tsv
├── ...
├── preprocessed_features.tsv
└── pdb_id_prediction.tsv
```
---

## 2. Predict in notebook
A Jupyter notebook (`Predictor.ipynb`) is included in the `Predictor` folder to demonstrate and run the software interactively.

You can choose one of the following options:

#### Option 1: Run on Google Colab (Recommended)

Run the notebook directly in the browser without any setup:

- Open it in [Google Colab](https://colab.research.google.com/)
- All dependencies will be automatically installed in the Colab session

Alternatively, you can follow the launch badge below:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enric-bazz/protein-contact-classification/master/Predictor/Predictor.ipynb)

#### Option 2: Run Locally

To run the notebook on your own machine:

1. Create a Python environment with the required dependencies by following the `requirements.txt` or `environment.yml` files.
3. Install Jupyter (if not already installed)
   ```bash
   pip install notebook
   ```
4. Launch the notebook
   ```bash
   jupyter notebook
   ```
---

## Additional features script
If interested, one can also run the script used to calculate the additional features on top of the data provided. 

The script is placed in the same folder and is called `cal_additional_features.py`. It is internally called by `run_prediction.py`, thus it accepts the same input format, specifically:

**Input Types:**
1. Single TSV file
2. Directory containing TSV files
3. ZIP file containing TSV files

Usage:
```shell script
python calc_additional_features.py <input_path> [-out_dir OUTPUT_DIR]
```


**Output Files:**
1. **Individual TSV Files:**
   - One TSV file per PDB structure
   - Named as `{pdb_id}.tsv`
   - Contains original data plus additional calculated features:
     - `same_chain`: Boolean indicating if residues are in same chain
     - `delta_rsa`: Absolute difference in relative solvent accessibility
     - `delta_atchley_1` through `delta_atchley_5`: Absolute differences in Atchley factors
     - `ca_distance`: Distance between CA atoms of residue pairs
     - Centroid coordinates (`s_centroid_x`, `s_centroid_y`, `t_centroid_x`, `t_centroid_y`) if `states.txt` is present

2. **ZIP Archive:**
   - Name: `features_ring_extended.zip`
   - Contains all individual TSV files similarly to the `features_ring.zip` file

**Output Location:**
- If `-out_dir` is specified: All files are saved in that directory
- If `-out_dir` is not specified: Files are saved in current directory (`.`)

Example directory structure after running:
```
output_directory/
├── features_ring_extended.zip
├── 1abc.tsv
├── 2def.tsv
└── ...
```

> Note.
The script requires:
- `states.txt` file in the script directory
- Internet connection to download PDB files for CA distance calculations
